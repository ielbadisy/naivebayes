---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "docs/reference/figures/"
)
```


# naivebayes <img src="docs/reference/figures/logo.png" align="right" />

[![Build Status](https://travis-ci.org/majkamichal/naivebayes.svg?branch=master)](https://travis-ci.org/majkamichal/naivebayes)
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/naivebayes)](https://cran.r-project.org/package=naivebayes)
[![](http://cranlogs.r-pkg.org/badges/naivebayes)](http://cran.rstudio.com/web/packages/naivebayes/index.html)
[![Say Thanks:)](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/majkamichal)

## 1. Overview

The `naivebayes` package provides an efficient implementation of the popular Na&iuml;ve Bayes classifier in `R`. It was developed and is now maintained based on three principles: it should be efficient, user friendly and written in `Base R`. The last implies no dependencies, however, it neither denies nor interferes with being efficient as many functions from the `Base R` distribution use highly efficient routines programmed in lower level languages, such as `C` or `FORTRAN`. In fact, the `naivebayes` package utilizes only such functions for resource-intensive calculations. 

The general function `naive_bayes()` detects the class of each feature in the dataset and, depending on the user choices, assumes possibly different distribution for each feature. It currently supports following class conditional distributions: 

  - categorical distribution for discrete features
  - Poisson distribution for non-negative integers 
  - Gaussian distribution for continuous features 
  - non-parametrically estimated densities via Kernel Density Estimation for continuous features

In addition to that specialized functions are available which implement:

 - Bernoulli Naive Bayes via `bernoulli_naive_bayes()`
 - Multinomial Naive Bayes via `multinomial_naive_bayes()`
 - Poisson Naive Bayes via `poisson_naive_bayes()`
 - Gaussian Naive Bayes via `gaussian_naive_bayes()`
 - Non-Parametric Naive Bayes via `nonparametric_naive_bayes()`

They are implemented based on the linear algebra operations which makes them efficient on the dense matrices. In close future sparse matrices will be supported in order to boost the performance on the sparse data. Also few helper functions are provided that are supposed to improve the user experience. The general `naive_bayes()` function is also available through the excellent `Caret` package.

Extended documentation can be found on the website: https://majkamichal.github.io/naivebayes/  


## 2. Installation

Just like many other `R` packages, `naivebayes` can be installed from the `CRAN`  repository by simply executing in the console the following line:

```{r, eval = FALSE}
install.packages("naivebayes")

# Or the the development version from GitHub:
devtools::install_github("majkamichal/naivebayes")
```

## 3. Usage

The `naivebayes` package provides a user friendly implementation of the Na&iuml;ve Bayes algorithm via formula interlace and classical combination of the matrix/data.frame containing the features and a vector with the class labels. All functions can recognize missing values, give an informative warning and more importantly - they can handle them. In following the basic usage of the `naivebayes` package is demonstrated:


```{r example, cache=TRUE}
library(naivebayes)

data(iris)
new <- iris[-c(1,2,3)]
# Add one categorical and count variable
set.seed(1)
new$Discrete <- sample(LETTERS[1:3], nrow(new), TRUE) 
set.seed(1)
new$Counts <- c(rpois(50, 1), rpois(50, 2), rpois(50, 10)) 

# Formula interface
nb <- naive_bayes(Species ~ ., usepoisson = TRUE, data = new)
summary(nb)


# Or equivalently matrix/data.frame and class vector
df <- new[-2]
class_vec <- new[[2]]
nb2 <- naive_bayes(x = df, y = class_vec, usepoisson = TRUE)
nb2

# Visualize class conditional probability distributions
plot(nb, which = c("Petal.Width", "Discrete"),
     arg.cat = list(color = heat.colors(3)))

# Browse tables
tables(nb, which = "Discrete") # <=> nb$tables["Discrete"]

# Get name of conditional distributions for each feature
get_cond_dist(nb) # <=> attr(nb$tables, "cond_dist") 

# data.frame("Dist" = get_cond_dist(nb))

# Classification
head(predict(nb))

# Posterior probabilities
head(predict(nb, type = "prob"))
```

### 3.1 Specialized Naive Bayes

#### 3.1.1 Bernoulli Naive Bayes ("bernoulli_naive_bayes")

```{r example_bernoulli_naive_bayes, cache=TRUE}
### Simulate the data:
set.seed(1)
cols <- 10 ; rows <- 100 ; probs <- c("0" = 0.4, "1" = 0.1)
M <- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols)
y <- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) <- paste0("V", seq_len(ncol(M)))
laplace <- 0.5

### Train the Bernoulli Naive Bayes
bnb <- bernoulli_naive_bayes(x = M, y = y, laplace = laplace)
head(predict(bnb, newdata = M, type = "prob"))


###  Equivalent calculation with general naive_bayes function.
###  (it is made sure that the columns are factors with the 0-1 levels)

df <- as.data.frame(lapply(as.data.frame(M), factor, levels = c(0,1)))
# sapply(df, class)
nb <- naive_bayes(df, y, laplace = laplace)
head(predict(nb, type = "prob"))


# Obtain probability tables
tables(bnb, which = "V1")
tables(nb, "V1")

# Visualise class conditional Bernoulli distributions
plot(bnb, which = "V1")
plot(nb, "V1")

# Check the equivalence of the class conditional distributions
all(get_cond_dist(bnb) == get_cond_dist(nb))

# # Coerce the Bernoulli probability tables to a data.frame
# bernoulli_tables_to_df(bnb)

```


#### 3.1.1 Gaussian Naive Bayes ("gaussian_naive_bayes")

```{r, cache=TRUE}
### Simulate the data:
cols <- 4 ; rows <- 100 ; probs <- c("0" = 0.4, "1" = 0.1)
M <- matrix(rnorm(rows * cols), nrow = rows, ncol = cols)
y <- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE))
colnames(M) <- paste0("V", seq_len(ncol(M)))

### Train the Gaussian Naive Bayes
gnb <- gaussian_naive_bayes(x = M, y = y)
head(predict(gnb, newdata = M, type = "prob"))

###  Equivalent calculation with general naive_bayes function.
nb <- naive_bayes(M, y)
head(predict(nb, type = "prob"))

# Obtain probability tables
tables(gnb, which = "V1")
tables(nb, "V1")

# Visualise class conditional Gaussian distributions
plot(gnb, which = "V1")
```


### 3.2 Usage with Caret package ("naive_bayes")

```{r example_caret, cache=TRUE}
library(caret, quietly = TRUE)
library(naivebayes)

# Train the Naive Bayes model with the Caret package
naive_bayes_via_caret <- train(Species ~ ., 
                               data = new, 
                               method = "naive_bayes", 
                               usepoisson = TRUE)

naive_bayes_via_caret

# Classification
head(predict(naive_bayes_via_caret, newdata = new))

# Posterior probabilities
head(predict(naive_bayes_via_caret, newdata = new, type = "prob"))

## Recover the naive_bayes object
nb_object <- naive_bayes_via_caret$finalModel
class(nb_object)
```

Define tuning grid, do resampling and find the "optimal" model:

```{r example_caret2, cache=TRUE}

# Define tuning grid 
nb_grid <-   expand.grid(usekernel = c(TRUE, FALSE),
                         laplace = c(0, 0.5, 1), 
                         adjust = c(0.75, 1, 1.25, 1.5))
# Fit the Naive Bayes model 
set.seed(2550)
naive_bayes_via_caret2 <- train(Species ~ ., data = new, 
                               method = "naive_bayes",
                               usepoisson = TRUE,
                               tuneGrid = nb_grid)
# Selected tuning parameters
naive_bayes_via_caret2$finalModel$tuneValue

## View the final naive_bayes model
# naive_bayes_via_caret2$finalModel

# Visualize the tuning process
plot(naive_bayes_via_caret2)

# Perform classification 
head(predict(naive_bayes_via_caret2, newdata = new))
```


### 3.3 Usage with nproc package ("naive_bayes")

Please find more information about the `nproc` package under: https://cran.r-project.org/web/packages/nproc/


```{r example_nproc, cache = FALSE}
library(nproc)
library(naivebayes)

# Simulate data
set.seed(2550)
n <- 1000
x <- matrix(rnorm(n * 2), n, 2)
c <- 1 + 3 * x[ ,1]
y <- rbinom(n, 1, 1 / (1 + exp(-c)))
xtest <- matrix(rnorm(n * 2), n, 2)
ctest <- 1 + 3 * xtest[,1]
ytest <- rbinom(n, 1, 1 / (1 + exp(-ctest)))


# Use Naive Bayes classifier and the default type I error control with alpha=0.05
naive_bayes_via_nproc <- npc(x, y, method = "nb")

## Recover the "naive_bayes" object
# naive_bayes_via_nproc$fits[[1]]$fit

# Classification
nb_pred <- predict(naive_bayes_via_nproc, xtest)

# head(nb_pred$pred.label)

# Obtain various measures
accuracy <- mean(nb_pred$pred.label == ytest)
ind0 <- which(ytest == 0)
ind1 <- which(ytest == 1)
typeI <- mean(nb_pred$pred.label[ind0] != ytest[ind0]) #type I error on test set
typeII <- mean(nb_pred$pred.label[ind1] != ytest[ind1]) #type II error on test set

cat(" Overall Accuracy: ",  accuracy,"\n", 
    "Type I error:     ", typeI, "\n",
    "Type II error:    ", typeII, "\n")
```



### 3.4 Usage with superml package ("naive_bayes")

Please find more information about the `superml` package under: https://cran.r-project.org/web/packages/superml/

```{r example_superml, cache=TRUE}
library(superml)
data(iris)
naive_bayes_via_superml <- NBTrainer$new()
naive_bayes_via_superml$fit(iris, 'Species')

## Recover the naive_bayes object
# naive_bayes_via_superml$model

# Classification
head(naive_bayes_via_superml$predict(iris))

# Posterior probabilites
head(naive_bayes_via_superml$predict(iris, type = "prob"))


```



